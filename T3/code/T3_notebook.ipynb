{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy.stats import norm\n",
    "import itertools\n",
    "from utils import load_jester\n",
    "from utils import NormLogCDF\n",
    "\n",
    "# import jtplot\n",
    "from jupyterthemes import jtplot\n",
    "\n",
    "# you can select an alternative theme's plot style by name\n",
    "# oceans16 | grade3 | chesterish | onedork | monokai | solarizedl\n",
    "jtplot.style('onedork')\n",
    "\n",
    "# set \"context\" (paper, notebook, talk, or poster)\n",
    "# & font scale (scalar applied to labels, legend, etc.)\n",
    "jtplot.style('grade3', context='paper', fscale=1.4)\n",
    "\n",
    "# turn on X- and Y-axis tick marks (default=False)\n",
    "# and turn off the axis grid lines (default=True)\n",
    "jtplot.style(ticks=True, grid=False)\n",
    "\n",
    "# set the default figure size\n",
    "# x (length), y (height)\n",
    "jtplot.figsize(x=6., y=5.)\n",
    "\n",
    "# or just adjust the aspect ratio\n",
    "# new_length = length * aspect\n",
    "jtplot.figsize(aspect=1.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w = Variable(torch.from_numpy(np.array([2, -2, -2, -8, -2, 3, -2, 1]).reshape(8, -1)).double(), requires_grad = True)\n",
    "edges = Variable(torch.from_numpy(np.array([[0, 0, 0, 0, 0, 0, 0, 0], \\\n",
    "                                            [0, 0, 0, 0, 0, 1, 0, 0], \\\n",
    "                                            [0, 0, 0, 0, 0, 1, 0, 0], \\\n",
    "                                            [0, 0, 0, 0, 1, 1, 1, 1], \\\n",
    "                                            [0, 0, 0, 1, 0, 0, 0, 0], \\\n",
    "                                            [0, 1, 1, 1, 0, 0, 0, 0], \\\n",
    "                                            [0, 0, 0, 1, 0, 0, 0, 0], \\\n",
    "                                            [0, 0, 0, 1, 0, 0, 0, 0]])).double())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def global_score(w, edges, seen_ind):\n",
    "    return torch.exp(torch.mm(torch.transpose(w, 0, 1), seen_ind) + torch.mm(torch.transpose(seen_ind, 0, 1), torch.mm(edges, seen_ind)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 148.4132\n",
       "[torch.DoubleTensor of size 1x1]"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seen_ind = Variable(torch.from_numpy(np.array([1, 0, 0, 0, 0, 1, 0, 0]).reshape(w.size()[0], -1)).double())\n",
    "global_score(w, edges, seen_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def log_partition(w, edges):\n",
    "    all_possibilities = itertools.product([0, 1], repeat = 8)\n",
    "    sum_over_scores = Variable(torch.zeros(1).double())\n",
    "    for i in all_possibilities:\n",
    "        possibility = Variable(torch.from_numpy(np.array(i).reshape(len(i), -1)).double())\n",
    "        sum_over_scores = torch.add(sum_over_scores, global_score(w, edges, possibility))\n",
    "        \n",
    "    return torch.log(sum_over_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 3423.3716\n",
      "[torch.DoubleTensor of size 1x1]\n",
      "\n",
      "Variable containing:\n",
      "1.00000e-02 *\n",
      "  4.3353\n",
      "[torch.DoubleTensor of size 1x1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sum_over_universe = log_partition(w, edges)\n",
    "score = global_score(w, edges, seen_ind)\n",
    "print(torch.exp(sum_over_universe))\n",
    "print(score/torch.exp(sum_over_universe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0.9848\n",
       "[torch.DoubleTensor of size 1x1]"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_marginal(marginal, w, edges):\n",
    "    all_possibilities = itertools.product([0, 1], repeat = 8)\n",
    "    sum_scores = Variable(torch.zeros(1).double())\n",
    "    for i in all_possibilities:\n",
    "        seen_ind = Variable(torch.from_numpy(np.array(i).reshape(len(i), -1)).double())\n",
    "        if seen_ind[marginal].data.numpy()[0] == 1:\n",
    "            sum_scores = torch.add(sum_scores, global_score(w, edges, seen_ind))\n",
    "            \n",
    "    return sum_scores/torch.exp(sum_over_universe)\n",
    "\n",
    "get_marginal(5, w, edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0.8808\n",
       " 0.4942\n",
       " 0.4942\n",
       " 0.0412\n",
       " 0.1349\n",
       " 0.9848\n",
       " 0.1349\n",
       " 0.7402\n",
       "[torch.DoubleTensor of size 8x1]"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_part = log_partition(w, edges)\n",
    "log_part.backward()\n",
    "w.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Marginals: [ 0.88079708  0.49420107  0.49420107  0.04124983  0.13491074  0.9847716\n",
      "  0.13491074  0.74019606]\n"
     ]
    }
   ],
   "source": [
    "up_exp = np.exp([2, -2, -2, -8, -2, 3, -2, 1])\n",
    "bp_exp = np.exp(2)\n",
    "root = 2\n",
    "edges = [[2, 5], [5, 1, 3], [3, 7, 4, 6]]\n",
    "leaves = np.array([0, 1, 4, 6, 7])\n",
    "\n",
    "upwards_bel = np.zeros((len(up_exp), 2))\n",
    "bel = np.zeros((len(up_exp), 2))\n",
    "upwards_msg = np.zeros((len(up_exp), 2))\n",
    "downwards_msg = np.zeros((len(up_exp), 2))\n",
    "\n",
    "for i in leaves: \n",
    "    upwards_bel[i, 0] = 1\n",
    "    upwards_bel[i, 1] = up_exp[i]\n",
    "\n",
    "for i in np.flip(edges, axis = 0): \n",
    "    parent = i[0]\n",
    "    child = i[1:]\n",
    "\n",
    "    for k in child: \n",
    "        upwards_msg[k, 0] = np.sum(upwards_bel[k,:])\n",
    "        upwards_msg[k, 1] = upwards_bel[k, 0] + bp_exp * upwards_bel[k, 1]\n",
    "\n",
    "    upwards_bel[parent, 0] = np.prod(upwards_msg[child, 0])\n",
    "    upwards_bel[parent, 1] = up_exp[parent] * np.prod(upwards_msg[child, 1])\n",
    "    \n",
    "bel[root, :] = upwards_bel[root, :]\n",
    "bel[0, :] = upwards_bel[0, :]\n",
    "        \n",
    "for i in edges: \n",
    "    parent = i[0]\n",
    "    child = i[1:]\n",
    "    for k in child: \n",
    "\n",
    "        downwards_msg[k, 0] = np.sum(bel[parent, :] / upwards_msg[k, :])\n",
    "        downwards_msg[k, 1] = bel[parent, 0] / upwards_msg[k, 0] + bp_exp * bel[parent, 1] / upwards_msg[k, 1]\n",
    "\n",
    "        bel[k, 0] = upwards_bel[k, 0] * downwards_msg[k, 0]\n",
    "        bel[k, 1] = upwards_bel[k, 1] * downwards_msg[k, 1]\n",
    "\n",
    "normalized_bel = np.divide(bel, (bel[:, 0] + bel[:, 1]).reshape(len((bel[:, 0] + bel[:, 1])), -1))\n",
    "print('Marginals: ' + str(normalized_bel[:, 1]))        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_joke_rating = []\n",
    "for line in open('jester_ratings.dat', 'r'):\n",
    "    user_joke_rating_string = line.rstrip()\n",
    "    user_joke_rating_floats = np.array(user_joke_rating_string.split(' ')).astype(np.float64)\n",
    "    user_joke_rating.append(user_joke_rating_floats)\n",
    "    \n",
    "user_joke_rating = np.array(user_joke_rating)\n",
    "highest_user = int(np.max(user_joke_rating[:, 0]))\n",
    "num_jokes = int(np.max(user_joke_rating[:, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data, this might take several minutes\n",
      "0 lines read\n",
      "100000 lines read\n",
      "200000 lines read\n",
      "300000 lines read\n",
      "400000 lines read\n",
      "500000 lines read\n",
      "600000 lines read\n",
      "700000 lines read\n",
      "800000 lines read\n",
      "900000 lines read\n",
      "1000000 lines read\n",
      "1100000 lines read\n",
      "1200000 lines read\n",
      "1300000 lines read\n",
      "1400000 lines read\n",
      "1500000 lines read\n",
      "1600000 lines read\n",
      "1700000 lines read\n",
      "Data Loaded\n"
     ]
    }
   ],
   "source": [
    "batch_size = 3000\n",
    "train_iter, val_iter, test_iter, text_field = load_jester(ratings_path='jester_ratings.dat.gz', jokes_path='jester_items.clean.dat.gz', subsample_rate=1.0, batch_size=batch_size, \\\n",
    "                                                          gpu=False, repeat=False, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2 Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def neg_log_likelihood(sigma2, U, V, users, jokes, ratings):\n",
    "    constant = Variable((torch.ones(1) * np.log((2*np.pi*sigma2)**0.5)).double())\n",
    "    multiplier = Variable((torch.ones(1) * (2*sigma2)**(-1.0)).double())\n",
    "    return constant + multiplier * torch.sum((ratings.double() - torch.mm(U, torch.transpose(V, 0, 1))[users.data, jokes.data])**2)\n",
    "\n",
    "def neg_log_likelihood_biases(sigma2, U, V, a, b, g, users, jokes, ratings):\n",
    "    constant = Variable((torch.ones(1) * np.log((2*np.pi*sigma2)**0.5)).double())\n",
    "    multiplier = Variable((torch.ones(1) * (2*sigma2)**(-1.0)).double())\n",
    "    return constant + multiplier * torch.sum((ratings.double() - torch.mm(U, torch.transpose(V, 0, 1))[users.data, jokes.data] - a[users.data] - b[jokes.data] - g)**2)\n",
    "\n",
    "def rmse(U, V, data_iter):\n",
    "    squared_error = 0.0\n",
    "    num_ratings = 0\n",
    "    for batch in data_iter:\n",
    "        ratings = batch.ratings \n",
    "        users = batch.users-1 \n",
    "        jokes = batch.jokes-1\n",
    "        \n",
    "        num_ratings += ratings.size()[0]\n",
    "        squared_error += torch.sum((ratings.double() - torch.mm(U, torch.transpose(V, 0, 1))[users.data, jokes.data])**2)\n",
    "        \n",
    "    return ((squared_error/num_ratings)**0.5).data.numpy()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manual gradient with respect to A:\n",
      "[[  1.   1.]\n",
      " [ 12.  12.]\n",
      " [ 18.  18.]]\n",
      "\n",
      "Autograd gradient with respect to A:\n",
      "Variable containing:\n",
      "  1   1\n",
      " 12  12\n",
      " 18  18\n",
      "[torch.DoubleTensor of size 3x2]\n",
      "\n",
      "\n",
      "Manual gradient with respect to B:\n",
      "[[  1.   1.]\n",
      " [ 39.  39.]]\n",
      "\n",
      "Autograd gradient with respect to B:\n",
      "Variable containing:\n",
      "  1   1\n",
      " 39  39\n",
      "[torch.DoubleTensor of size 2x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Toy example to do manual calculation of gradients\n",
    "A = Variable(torch.from_numpy(np.array([[1, 1], [2, 2], [3, 3]])).double(), requires_grad = True)\n",
    "B = Variable(torch.from_numpy(np.array([[1, 1], [2, 2]])).double(), requires_grad = True)\n",
    "person_ids = Variable(torch.from_numpy(np.array([0, 1, 2])).long(), requires_grad = True)\n",
    "song_ids = Variable(torch.from_numpy(np.array([0, 1, 1])).long(), requires_grad = True)\n",
    "rate = Variable(torch.from_numpy(np.array([1, 2, 3])).long())\n",
    "sigma2 = 1.0\n",
    "\n",
    "neg_log_lik = neg_log_likelihood(sigma2, A, B, person_ids, song_ids, rate)\n",
    "neg_log_lik.backward()\n",
    "\n",
    "A_num = A.data.numpy()\n",
    "B_num = B.data.numpy()\n",
    "rate_matrix = np.array([[1, 0], [0, 2], [0, 3]])\n",
    "mult = (-torch.ones(1) * (sigma2)**(-1.0)).numpy()\n",
    "\n",
    "a1_grad = mult * (rate_matrix[0, 0] - A_num[0].reshape(-1, A_num.shape[1]).dot(B_num[0].reshape(B_num.shape[1], -1))) * B_num[0]\n",
    "a2_grad = mult * (rate_matrix[1, 1] - A_num[1].reshape(-1, A_num.shape[1]).dot(B_num[1].reshape(B_num.shape[1], -1))) * B_num[1]\n",
    "a3_grad = mult * (rate_matrix[2, 1] - A_num[2].reshape(-1, A_num.shape[1]).dot(B_num[1].reshape(B_num.shape[1], -1))) * B_num[1]\n",
    "A_grad = np.concatenate([a1_grad, a2_grad, a3_grad], axis = 0)\n",
    "\n",
    "b1_grad = mult * (rate_matrix[0, 0] - A_num[0].reshape(-1, A_num.shape[1]).dot(B_num[0].reshape(B_num.shape[1], -1))) * A_num[0]\n",
    "b2_grad = mult * ((rate_matrix[1, 1] - A_num[1].reshape(-1, A_num.shape[1]).dot(B_num[1].reshape(B_num.shape[1], -1))) * A_num[1] +\\\n",
    "                   (rate_matrix[2, 1] - A_num[2].reshape(-1, A_num.shape[1]).dot(B_num[1].reshape(B_num.shape[1], -1))) * A_num[2])\n",
    "B_grad = np.concatenate([b1_grad, b2_grad], axis = 0)\n",
    "\n",
    "print('Manual gradient with respect to A:\\n' + str(A_grad))\n",
    "print()\n",
    "print('Autograd gradient with respect to A:\\n' + str(A.grad))\n",
    "print()\n",
    "print('Manual gradient with respect to B:\\n' + str(B_grad))\n",
    "print()\n",
    "print('Autograd gradient with respect to B:\\n' + str(B.grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Batch number 100\n",
      "Batch number 200\n",
      "Batch number 300\n",
      "Batch number 400\n",
      "Batch number 500\n",
      "Epoch 1\n",
      "Batch number 100\n",
      "Batch number 200\n",
      "Batch number 300\n",
      "Batch number 400\n",
      "Batch number 500\n",
      "Epoch 2\n",
      "Batch number 100\n",
      "Batch number 200\n",
      "Batch number 300\n",
      "Batch number 400\n",
      "Batch number 500\n",
      "Epoch 3\n",
      "Batch number 100\n",
      "Batch number 200\n",
      "Batch number 300\n",
      "Batch number 400\n",
      "Batch number 500\n",
      "Epoch 4\n",
      "Batch number 100\n",
      "Batch number 200\n",
      "Batch number 300\n",
      "Batch number 400\n",
      "Batch number 500\n",
      "Sigma squared MLE = 1.04658868677\n",
      "Train RMSE: 1.023029172\n",
      "Val RMSE: 1.03897252666\n"
     ]
    }
   ],
   "source": [
    "# batch size 3000\n",
    "K = 9\n",
    "sigma2 = 1.0\n",
    "num_epochs = 5\n",
    "#U = Variable(torch.from_numpy(np.random.rand(highest_user, K).reshape(highest_user, K)), requires_grad = True)\n",
    "#V = Variable(torch.from_numpy(np.random.rand(num_jokes, K).reshape(num_jokes, K)), requires_grad = True)\n",
    "#optimizer = torch.optim.SGD([U, V], lr = 0.0004)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print('Epoch ' + str(epoch))\n",
    "    batch_num = 0\n",
    "    for batch in train_iter:\n",
    "        batch_num += 1\n",
    "        if batch_num % 100 == 0:\n",
    "            print('Batch number ' + str(batch_num))\n",
    "\n",
    "        ratings = batch.ratings\n",
    "        users = batch.users-1 \n",
    "        jokes = batch.jokes-1\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = neg_log_likelihood(sigma2, U, V, users, jokes, ratings)\n",
    "        loss.backward()\n",
    "        #print(loss.data[0])\n",
    "        optimizer.step()\n",
    "        \n",
    "\n",
    "train_rmse_k = rmse(U, V, train_iter)\n",
    "val_rmse_k = rmse(U, V, val_iter)\n",
    "\n",
    "sigma2_mle = train_rmse_k**2\n",
    "\n",
    "print('Sigma squared MLE = ' + str(sigma2_mle))\n",
    "print('Train RMSE: ' + str(train_rmse_k))\n",
    "print('Val RMSE: ' + str(val_rmse_k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sigma2 = 1.0\n",
    "K = np.arange(10)\n",
    "optimizer = torch.optim.SGD([U, V], lr = 0.0005)\n",
    "num_epochs = 10\n",
    "train_rmse = []\n",
    "val_rmse = []\n",
    "\n",
    "for k in K:\n",
    "    print('K = ' + str(k+1))\n",
    "    U = Variable(torch.from_numpy(np.random.rand(highest_user, k+1).reshape(highest_user, k+1)), requires_grad = True)\n",
    "    V = Variable(torch.from_numpy(np.random.rand(num_jokes, k+1).reshape(num_jokes, k+1)), requires_grad = True)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch ' + str(epoch))\n",
    "        batch_num = 0\n",
    "        for batch in train_iter:\n",
    "            batch_num += 1\n",
    "            if batch_num % 100 == 0:\n",
    "                print('Batch number ' + str(batch_num))\n",
    "\n",
    "            ratings = batch.ratings \n",
    "            users = batch.users-1 \n",
    "            jokes = batch.jokes-1\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss = neg_log_likelihood(sigma2, U, V, users, jokes, ratings)\n",
    "            loss.backward()\n",
    "            print(loss.data[0])\n",
    "            optimizer.step()\n",
    "            \n",
    "    train_rmse_k = rmse(U, V, train_iter)\n",
    "    val_rmse_k = rmse(U, V, val_iter)\n",
    "    \n",
    "    print('Train RMSE: ' + str(train_rmse_k))\n",
    "    print('Val RMSE: ' + str(val_rmse_k))\n",
    "    \n",
    "    train_rmse.append(train_rmse_k)\n",
    "    val_rmse.append(val_rmse_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "[1.1039178662337159, 1.1282048470046748] #lr = 0.0008, batch_size=3000, k=1\n",
    "[1.0573551910309043, 1.0920181002235005] #lr = 0.0008, batch_size=3000, k=2\n",
    "[1.04716172563, 1.06281632165] #lr = 0.0008, batch_size=3000, k=3\n",
    "[1.04378780926, 1.05934880586] #lr = 0.0005, batch_size=3000, k=4\n",
    "[1.04299195183, 1.05527209921] #lr = 0.0005, batch_size=3000, k=5\n",
    "[1.03386666602, 1.04849306749] #lr = 0.0005, batch_size=3000, k=6\n",
    "[1.03170220691, 1.04641087284] #lr = 0.0005, batch_size=3000, k=7\n",
    "[1.02506414753, 1.04296469044] #lr = 0.0005, batch_size=3000, k=8\n",
    "[1.023029172, 1.03897252666] #lr = 0.0004, batch_size=3000, k=9\n",
    "[1.021032441, 1.03527865811] #lr = 0.0004, batch_size=3000, k=10\n",
    "\n",
    "K = np.arange(1, 11)\n",
    "train_rmse = [1.1039178662337159, 1.0573551910309043, 1.04716172563, 1.04378780926, 1.04299195183, 1.03386666602, 1.03170220691, 1.02506414753, 1.023029172, 1.021032441]\n",
    "val_rmse = [1.1282048470046748, 1.0920181002235005, 1.06281632165, 1.05934880586, 1.05527209921, 1.04849306749, 1.04641087284, 1.04296469044, 1.03897252666, 1.03527865811]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Christopher\\Anaconda3\\envs\\py35\\lib\\site-packages\\matplotlib\\figure.py:1743: UserWarning: This figure includes Axes that are not compatible with tight_layout, so its results might be incorrect.\n",
      "  warnings.warn(\"This figure includes Axes that are not \"\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtUAAAEWCAYAAACpJ2vsAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x26bd96f7668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(K, train_rmse, 'ro')\n",
    "plt.plot(K, val_rmse, 'bo')\n",
    "plt.xlabel('K')\n",
    "plt.ylabel('RMSE')\n",
    "plt.title('RMSE vs. K for Training and Validation Data')\n",
    "plt.legend(labels = ['Training', 'Validation'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Batch number 100\n",
      "Batch number 200\n",
      "Batch number 300\n",
      "Batch number 400\n",
      "Batch number 500\n",
      "Epoch 1\n",
      "Batch number 100\n",
      "Batch number 200\n",
      "Batch number 300\n",
      "Batch number 400\n",
      "Batch number 500\n",
      "Epoch 2\n",
      "Batch number 100\n",
      "Batch number 200\n",
      "Batch number 300\n",
      "Batch number 400\n",
      "Batch number 500\n",
      "Epoch 3\n",
      "Batch number 100\n",
      "Batch number 200\n",
      "Batch number 300\n",
      "Batch number 400\n",
      "Batch number 500\n",
      "Epoch 4\n",
      "Batch number 100\n",
      "Batch number 200\n",
      "Batch number 300\n",
      "Batch number 400\n",
      "Batch number 500\n",
      "Epoch 5\n",
      "Batch number 100\n",
      "Batch number 200\n",
      "Batch number 300\n",
      "Batch number 400\n",
      "Batch number 500\n",
      "Epoch 6\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-232-5599352d1483>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Epoch '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mbatch_num\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain_iter\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m         \u001b[0mbatch_num\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbatch_num\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m100\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\py35\\lib\\site-packages\\torchtext\\data\\iterator.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    162\u001b[0m                 \u001b[0mminibatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreverse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m                 yield Batch(minibatch, self.dataset, self.device,\n\u001b[1;32m--> 164\u001b[1;33m                             self.train)\n\u001b[0m\u001b[0;32m    165\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\py35\\lib\\site-packages\\torchtext\\data\\batch.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, dataset, device, train)\u001b[0m\n\u001b[0;32m     20\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mfield\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m                     setattr(self, name, field.numericalize(\n\u001b[1;32m---> 22\u001b[1;33m                         \u001b[0mfield\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m                         device=device, train=train))\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\py35\\lib\\site-packages\\torchtext\\data\\field.py\u001b[0m in \u001b[0;36mpad\u001b[1;34m(self, minibatch)\u001b[0m\n\u001b[0;32m    120\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mmax_len\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m                 \u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meos_token\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meos_token\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 122\u001b[1;33m                 [self.pad_token] * max(0, max_len - len(x)))\n\u001b[0m\u001b[0;32m    123\u001b[0m             \u001b[0mlengths\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpadded\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_len\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minclude_lengths\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# batch size 1000\n",
    "K = 2\n",
    "sigma2 = 1.0\n",
    "num_epochs = 7\n",
    "U = Variable(torch.from_numpy(0.1 * np.random.randn(highest_user, K).reshape(highest_user, K)), requires_grad = True)\n",
    "V = Variable(torch.from_numpy(0.1 * np.random.randn(num_jokes, K).reshape(num_jokes, K)), requires_grad = True)\n",
    "a = Variable(torch.from_numpy(0.1 * np.random.randn(highest_user).reshape(highest_user, -1)), requires_grad = True)\n",
    "b = Variable(torch.from_numpy(0.1 * np.random.randn(num_jokes).reshape(num_jokes, -1)), requires_grad = True)\n",
    "g = Variable(torch.ones(1).double() * 3, requires_grad = True)\n",
    "optimizer = torch.optim.Adam([U, V, a, b, g], lr = 0.05)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print('Epoch ' + str(epoch))\n",
    "    batch_num = 0\n",
    "    for batch in train_iter:\n",
    "        batch_num += 1\n",
    "        if batch_num % 100 == 0:\n",
    "            print('Batch number ' + str(batch_num))\n",
    "\n",
    "        ratings = batch.ratings \n",
    "        users = batch.users-1 \n",
    "        jokes = batch.jokes-1\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = neg_log_likelihood_biases(sigma2, U, V, a, b, g, users, jokes, ratings)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "print('Global bias: ' + str(g.data.numpy()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Rank jokes and get biases\n",
    "jokeID_bias = np.concatenate([np.arange(1, num_jokes+1).reshape(num_jokes, -1), b.data.numpy()], axis = 1)\n",
    "jokeID_bias[jokeID_bias[:, 1].argsort()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "jokes_raw = []\n",
    "for line in open('jester_items.clean.dat', 'r'):\n",
    "    if len(line) > 5:\n",
    "        jokes_raw.append(line.rstrip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 163)"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_transform = CountVectorizer(input='content', encoding='utf-8', decode_error='strict', strip_accents=None, \\\n",
    "                                                                     lowercase=True, preprocessor=None, tokenizer=None, stop_words=None, \\\n",
    "                                                                     ngram_range=(1, 1), analyzer='word', max_df=1.0, min_df=0.045, max_features=None, \\\n",
    "                                                                     vocabulary=None, binary=False)\n",
    "\n",
    "joke_feature_vectors = raw_transform.fit_transform(jokes_raw).todense()\n",
    "joke_feature_vectors = np.concatenate([np.ones(num_jokes).reshape(num_jokes, -1), joke_feature_vectors], axis = 1)\n",
    "joke_feature_vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3 Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neg_log_posterior(w, x, sigma, b, y, jokes):\n",
    "    features = x[jokes.data.numpy()]\n",
    "    linear = features.dot(w.data.numpy().reshape(w.size()[0], -1))\n",
    "    \n",
    "    lower_cut = b[y.data.numpy() - 1].reshape(y.size()[0], -1)\n",
    "    upper_cut = lower_cut + 2\n",
    "    np.place(upper_cut, upper_cut == 6, 10.0)\n",
    "    np.place(lower_cut, lower_cut == -4, -10.0)\n",
    "    \n",
    "    lower_cdf_input = (lower_cut - linear)/sigma.data.numpy()\n",
    "    upper_cdf_input = (upper_cut - linear)/sigma.data.numpy()\n",
    "    \n",
    "    lower_cdf = norm.cdf(lower_cdf_input, loc=0, scale=1)\n",
    "    upper_cdf = norm.cdf(upper_cdf_input, loc=0, scale=1)\n",
    "\n",
    "    max1 = np.log(1 - np.exp(np.log(lower_cdf) - np.log(upper_cdf))) + np.log(upper_cdf)\n",
    "    max2 = np.log(1 - np.exp(np.log(1 - upper_cdf) - np.log(1 - lower_cdf))) + np.log(1 - lower_cdf)\n",
    "    max_matrix = np.concatenate([max1, max2], axis = 1)\n",
    "    max_log = Variable(torch.from_numpy(np.amax(max_matrix, axis = 1)).double())\n",
    "\n",
    "    return torch.sum(w**2) - torch.sum(max_log)\n",
    "\n",
    "\n",
    "\n",
    "def neg_log_gauss_likelihood(w, x, y, sigma):\n",
    "    return 0.5 * y.size()[0] * torch.log(2.0 * np.pi * (sigma**2)) + 0.5 * sigma**(-2) * torch.sum((y.double() - torch.mm(Variable(torch.from_numpy(np.array(x)).double()), w))**2)\n",
    "\n",
    "\n",
    "\n",
    "def rmse2(w, x, sigma, b, data_iter):\n",
    "    Xw = x.dot(w.data.numpy().reshape(w.size()[0], -1))\n",
    "    \n",
    "    cdf = np.ones((x.shape[0], len(b) + 1))\n",
    "    for i in range(cdf.shape[1]):\n",
    "        if i == 0:\n",
    "            cdf[:, i] = norm.cdf((cdf[:, i].reshape(cdf.shape[0], -1) * -10.0 - Xw)/sigma.data.numpy()).reshape(cdf.shape[0], )\n",
    "        elif i == (cdf.shape[1] - 1):\n",
    "            cdf[:, i] = norm.cdf((cdf[:, i].reshape(cdf.shape[0], -1) * 10.0 - Xw)/sigma.data.numpy()).reshape(cdf.shape[0], )\n",
    "        else:\n",
    "            cdf[:, i] = norm.cdf((cdf[:, i].reshape(cdf.shape[0], -1) * b[i] - Xw)/sigma.data.numpy()).reshape(cdf.shape[0], )\n",
    "            \n",
    "    prob_matrix = np.zeros((x.shape[0], len(b)))\n",
    "    for i in range(prob_matrix.shape[1]):\n",
    "        prob_matrix[:, i] = cdf[:, i+1] - cdf[:, i]\n",
    "        \n",
    "    possible_ratings = (np.arange(5) + 1).reshape(5, -1)\n",
    "    \n",
    "    expectation_by_joke = prob_matrix.dot(possible_ratings)\n",
    "    \n",
    "    squared_error = 0.0\n",
    "    num_ratings = 0\n",
    "    for batch in data_iter:\n",
    "        ratings = batch.ratings-1\n",
    "        users = batch.users-1 \n",
    "        jokes = batch.jokes-1\n",
    "            \n",
    "        \n",
    "        num_ratings += ratings.data.numpy().shape[0]\n",
    "        squared_error += np.sum((expectation_by_joke[jokes.data.numpy()] - ratings.data.numpy().reshape(ratings.size()[0], -1))**2)\n",
    "        \n",
    "    return (squared_error/num_ratings)**0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Batch number 100\n",
      "Batch number 200\n",
      "Batch number 300\n",
      "Batch number 400\n",
      "Batch number 500\n",
      "Test RMSE: 1.30168070677\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 1\n",
    "b = np.array([-4, -2, 0, 2, 4])\n",
    "#w = Variable(torch.from_numpy(0.1 * np.random.randn(joke_feature_vectors.shape[1]).reshape(joke_feature_vectors.shape[1], -1)), requires_grad = True)\n",
    "#sigma = Variable(5 * torch.from_numpy(np.random.rand(1)), requires_grad = True)\n",
    "#optimizer = torch.optim.Adam([w, sigma], lr = 0.005)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print('Epoch ' + str(epoch))\n",
    "    batch_num = 0\n",
    "    for batch in train_iter:\n",
    "        batch_num += 1\n",
    "        if batch_num % 100 == 0:\n",
    "            print('Batch number ' + str(batch_num))\n",
    "\n",
    "        ratings = batch.ratings-1 \n",
    "        users = batch.users-1 \n",
    "        jokes = batch.jokes-1\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = neg_log_posterior(w, joke_feature_vectors, sigma, b, ratings, jokes)\n",
    "        loss.backward()\n",
    "        #print(loss.data[0])\n",
    "        optimizer.step()\n",
    "\n",
    "print('Test RMSE: ' + str(rmse2(w, joke_feature_vectors, sigma, b, test_iter)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 6.5794\n",
       "[torch.DoubleTensor of size 1]"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigma**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 2.1116e-28\n",
       "-1.0308e-16\n",
       "-1.4834e-29\n",
       " 2.1508e-28\n",
       "-1.3963e-30\n",
       "-5.6252e-23\n",
       "-2.8840e-15\n",
       "-5.4405e-28\n",
       " 2.6699e-28\n",
       "-6.0757e-29\n",
       " 9.5374e-25\n",
       " 4.2562e-25\n",
       " 3.2805e-27\n",
       "-1.1288e-28\n",
       "-1.0367e-28\n",
       " 5.3538e-27\n",
       " 7.5423e-28\n",
       " 6.8906e-27\n",
       " 2.4212e-28\n",
       " 1.6025e-27\n",
       " 1.9136e-27\n",
       " 6.9395e-11\n",
       "-5.3347e-23\n",
       " 1.2013e-27\n",
       " 8.2377e-30\n",
       "-4.0415e-27\n",
       "-1.2174e-27\n",
       " 9.4410e-28\n",
       "-3.0547e-30\n",
       "-1.5727e-27\n",
       " 8.3317e-08\n",
       "-2.2609e-28\n",
       " 8.8179e-26\n",
       " 5.0226e-29\n",
       " 1.1196e-27\n",
       "-1.3341e-28\n",
       "-1.5650e-28\n",
       "-4.4472e-23\n",
       "-4.3138e-09\n",
       " 1.1640e-26\n",
       "-7.3984e-29\n",
       " 7.1223e-27\n",
       "-2.7051e-27\n",
       "-8.1352e-26\n",
       "-8.1057e-27\n",
       " 4.8437e-28\n",
       "-6.2097e-29\n",
       "-1.0371e-10\n",
       " 6.1987e-28\n",
       "-3.4184e-18\n",
       "-1.1628e-27\n",
       " 1.4036e-28\n",
       " 1.1711e-27\n",
       "-1.7286e-07\n",
       "-1.8521e-29\n",
       " 2.9590e-28\n",
       " 3.5942e-15\n",
       "-2.8348e-19\n",
       "-3.3798e-29\n",
       " 1.6894e-18\n",
       " 9.8680e-21\n",
       "-7.4688e-31\n",
       "-5.3513e-28\n",
       "-7.4957e-28\n",
       " 1.9887e-29\n",
       "-1.8535e-23\n",
       "-2.4486e-24\n",
       "-2.8668e-28\n",
       " 1.8600e-23\n",
       "-5.2279e-21\n",
       " 2.2171e-24\n",
       "-2.0490e-29\n",
       "-1.0312e-24\n",
       " 1.3340e-28\n",
       "-1.0207e-21\n",
       " 2.7474e-27\n",
       " 2.8038e-27\n",
       " 2.7497e-09\n",
       "-1.6223e-26\n",
       " 3.5194e-28\n",
       "-8.9755e-17\n",
       " 3.5600e-29\n",
       " 2.4333e-28\n",
       "-1.0462e-27\n",
       " 3.7448e-22\n",
       "-5.3546e-21\n",
       " 8.5839e-28\n",
       " 2.6583e-28\n",
       "-3.4819e-28\n",
       "-2.9454e-29\n",
       "-3.1454e-28\n",
       "-9.0826e-26\n",
       "-3.6560e-28\n",
       " 1.1285e-28\n",
       "-2.3598e-28\n",
       "-1.4975e-26\n",
       "-1.1217e-27\n",
       "-2.4083e-28\n",
       "-1.7905e-23\n",
       " 2.3561e-25\n",
       " 4.0022e-08\n",
       "-2.0972e-29\n",
       " 1.7625e-23\n",
       "-6.6567e-29\n",
       "-3.1613e-28\n",
       "-9.2698e-11\n",
       " 1.0426e-10\n",
       " 2.9313e-24\n",
       "-5.4867e-23\n",
       " 6.7893e-27\n",
       "-2.0910e-27\n",
       "-1.5861e-28\n",
       "-1.9627e-25\n",
       "-4.8640e-26\n",
       " 2.5109e-20\n",
       "-2.8539e-27\n",
       " 7.3039e-28\n",
       " 3.3424e-28\n",
       "-4.4586e-25\n",
       "-8.8696e-22\n",
       " 1.2661e-28\n",
       " 4.0666e-30\n",
       " 1.2339e-18\n",
       "-8.9456e-29\n",
       "-3.7486e-28\n",
       "-3.6455e-27\n",
       "-7.7846e-28\n",
       " 1.0433e-24\n",
       " 1.2583e-27\n",
       "-5.5791e-29\n",
       " 8.2660e-30\n",
       "-2.5144e-17\n",
       "-6.1850e-30\n",
       " 6.7540e-27\n",
       " 1.3355e-08\n",
       "-1.1355e-29\n",
       " 5.9214e-27\n",
       " 3.3451e-27\n",
       " 3.0260e-09\n",
       "-1.1990e-26\n",
       " 7.5970e-22\n",
       "-2.2774e-13\n",
       " 2.0166e-24\n",
       "-5.7185e-28\n",
       "-1.6015e-28\n",
       " 1.0031e-25\n",
       " 3.8563e-09\n",
       " 3.8764e-26\n",
       "-4.2684e-18\n",
       " 5.8681e-29\n",
       " 5.4776e-16\n",
       " 4.1298e-27\n",
       " 6.3310e-24\n",
       "-4.3561e-29\n",
       "-1.0799e-28\n",
       " 4.7038e-28\n",
       "-7.5122e-26\n",
       " 3.5160e-28\n",
       " 3.4845e-28\n",
       "-3.5321e-20\n",
       "-8.1774e-26\n",
       "-3.5277e-29\n",
       "[torch.DoubleTensor of size 162x1]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Batch number 100\n",
      "Batch number 200\n",
      "Batch number 300\n",
      "Batch number 400\n",
      "Batch number 500\n",
      "Test RMSE: 1.31696375464\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "b = np.array([-4, -2, 0, 2, 4])\n",
    "w = Variable(torch.from_numpy(0.1 * np.random.randn(joke_feature_vectors.shape[1]).reshape(joke_feature_vectors.shape[1], -1)), requires_grad = True)\n",
    "sigma = Variable(torch.from_numpy(np.array(np.random.rand(1))), requires_grad = True)\n",
    "optimizer = torch.optim.Adam([w, sigma], lr = 0.05)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print('Epoch ' + str(epoch))\n",
    "    batch_num = 0\n",
    "    for batch in train_iter:\n",
    "        batch_num += 1\n",
    "        if batch_num % 100 == 0:\n",
    "            print('Batch number ' + str(batch_num))\n",
    "\n",
    "        ratings = batch.ratings-1 \n",
    "        users = batch.users-1\n",
    "        jokes = batch.jokes-1\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = neg_log_gauss_likelihood(w, joke_feature_vectors, ratings, sigma)\n",
    "        loss.backward()\n",
    "        #print(loss.data[0])\n",
    "        optimizer.step()\n",
    "\n",
    "print('Test RMSE: ' + str(rmse2(w, joke_feature_vectors, sigma, b, test_iter)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5.3",
   "language": "python",
   "name": "py35"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
