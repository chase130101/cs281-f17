{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy.stats import norm\n",
    "import itertools\n",
    "from utils import load_jester\n",
    "from utils import NormLogCDF\n",
    "\n",
    "# import jtplot\n",
    "from jupyterthemes import jtplot\n",
    "\n",
    "# you can select an alternative theme's plot style by name\n",
    "# oceans16 | grade3 | chesterish | onedork | monokai | solarizedl\n",
    "jtplot.style('onedork')\n",
    "\n",
    "# set \"context\" (paper, notebook, talk, or poster)\n",
    "# & font scale (scalar applied to labels, legend, etc.)\n",
    "jtplot.style('grade3', context='paper', fscale=1.4)\n",
    "\n",
    "# turn on X- and Y-axis tick marks (default=False)\n",
    "# and turn off the axis grid lines (default=True)\n",
    "jtplot.style(ticks=True, grid=False)\n",
    "\n",
    "# set the default figure size\n",
    "# x (length), y (height)\n",
    "jtplot.figsize(x=6., y=5.)\n",
    "\n",
    "# or just adjust the aspect ratio\n",
    "# new_length = length * aspect\n",
    "jtplot.figsize(aspect=1.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = Variable(torch.from_numpy(np.array([2, -2, -2, -8, -2, 3, -2, 1]).reshape(8, -1)).double(), requires_grad = True)\n",
    "edges = Variable(torch.from_numpy(np.array([[0, 0, 0, 0, 0, 0, 0, 0], \\\n",
    "                                            [0, 0, 0, 0, 0, 1, 0, 0], \\\n",
    "                                            [0, 0, 0, 0, 0, 1, 0, 0], \\\n",
    "                                            [0, 0, 0, 0, 1, 1, 1, 1], \\\n",
    "                                            [0, 0, 0, 1, 0, 0, 0, 0], \\\n",
    "                                            [0, 1, 1, 1, 0, 0, 0, 0], \\\n",
    "                                            [0, 0, 0, 1, 0, 0, 0, 0], \\\n",
    "                                            [0, 0, 0, 1, 0, 0, 0, 0]])).double())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "def global_score(w, edges, seen_ind):\n",
    "    return torch.exp(torch.mm(torch.transpose(w, 0, 1), seen_ind) + torch.mm(torch.transpose(seen_ind, 0, 1), torch.mm(edges, seen_ind)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 148.4132\n",
       "[torch.DoubleTensor of size 1x1]"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seen_ind = Variable(torch.from_numpy(np.array([1, 0, 0, 0, 0, 1, 0, 0]).reshape(w.size()[0], -1)).double())\n",
    "global_score(w, edges, seen_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def log_partition(w, edges):\n",
    "    all_possibilities = itertools.product([0, 1], repeat = 8)\n",
    "    sum_over_scores = Variable(torch.zeros(1).double())\n",
    "    for i in all_possibilities:\n",
    "        possibility = Variable(torch.from_numpy(np.array(i).reshape(len(i), -1)).double())\n",
    "        sum_over_scores = torch.add(sum_over_scores, global_score(w, edges, possibility))\n",
    "        \n",
    "    return torch.log(sum_over_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 3423.3716\n",
      "[torch.DoubleTensor of size 1x1]\n",
      "\n",
      "Variable containing:\n",
      "1.00000e-02 *\n",
      "  4.3353\n",
      "[torch.DoubleTensor of size 1x1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sum_over_universe = log_partition(w, edges)\n",
    "score = global_score(w, edges, seen_ind)\n",
    "print(torch.exp(sum_over_universe))\n",
    "print(score/torch.exp(sum_over_universe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0.9848\n",
       "[torch.DoubleTensor of size 1x1]"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_possibilities = itertools.product([0, 1], repeat = 8)\n",
    "sum_scores = Variable(torch.zeros(1).double())\n",
    "for i in all_possibilities:\n",
    "    seen_ind = Variable(torch.from_numpy(np.array(i).reshape(len(i), -1)).double())\n",
    "    if seen_ind[5].data.numpy()[0] == 1:\n",
    "        sum_scores = torch.add(sum_scores, global_score(w, edges, seen_ind))\n",
    "        \n",
    "\n",
    "sum_scores/torch.exp(sum_over_universe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0.8808\n",
       " 0.4942\n",
       " 0.4942\n",
       " 0.0412\n",
       " 0.1349\n",
       " 0.9848\n",
       " 0.1349\n",
       " 0.7402\n",
       "[torch.DoubleTensor of size 8x1]"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_part = log_partition(w, edges)\n",
    "log_part.backward()\n",
    "w.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_joke_rating = []\n",
    "for line in open('jester_ratings.dat', 'r'):\n",
    "    user_joke_rating_string = line.rstrip()\n",
    "    user_joke_rating_floats = np.array(user_joke_rating_string.split(' ')).astype(np.float64)\n",
    "    user_joke_rating.append(user_joke_rating_floats)\n",
    "    \n",
    "user_joke_rating = np.array(user_joke_rating)\n",
    "highest_user = int(np.max(user_joke_rating[:, 0]))\n",
    "num_jokes = int(np.max(user_joke_rating[:, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data, this might take several minutes\n",
      "For debugging, you can consider setting subsample_rate to a smaller value to use a subsampled dataset\n",
      "0 lines read\n",
      "100000 lines read\n",
      "200000 lines read\n",
      "300000 lines read\n",
      "400000 lines read\n",
      "500000 lines read\n",
      "600000 lines read\n",
      "700000 lines read\n",
      "800000 lines read\n",
      "900000 lines read\n",
      "1000000 lines read\n",
      "1100000 lines read\n",
      "1200000 lines read\n",
      "1300000 lines read\n",
      "1400000 lines read\n",
      "1500000 lines read\n",
      "1600000 lines read\n",
      "1700000 lines read\n",
      "Data Loaded\n"
     ]
    }
   ],
   "source": [
    "batch_size = 3000\n",
    "train_iter, val_iter, test_iter, text_field = load_jester(ratings_path='jester_ratings.dat.gz', jokes_path='jester_items.clean.dat.gz', subsample_rate=1.0, batch_size=batch_size, \\\n",
    "                                                          gpu=False, repeat=False, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2 Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def neg_log_likelihood(sigma2, U, V, users, jokes, ratings):\n",
    "    constant = Variable((torch.ones(1) * np.log((2*np.pi*sigma2)**0.5)).double())\n",
    "    multiplier = Variable((torch.ones(1) * (2*sigma2)**(-1.0)).double())\n",
    "    return constant + multiplier * torch.sum((ratings.double() - torch.mm(U, torch.transpose(V, 0, 1))[users.data, jokes.data])**2)\n",
    "\n",
    "def neg_log_likelihood_biases(sigma2, U, V, a, b, g, users, jokes, ratings):\n",
    "    constant = Variable((torch.ones(1) * np.log((2*np.pi*sigma2)**0.5)).double())\n",
    "    multiplier = Variable((torch.ones(1) * (2*sigma2)**(-1.0)).double())\n",
    "    return constant + multiplier * torch.sum((ratings.double() - torch.mm(U, torch.transpose(V, 0, 1))[users.data, jokes.data] - a[users.data] - b[jokes.data] - g)**2)\n",
    "\n",
    "def rmse(U, V, data_iter):\n",
    "    squared_error = 0.0\n",
    "    num_ratings = 0\n",
    "    for batch in data_iter:\n",
    "        ratings = batch.ratings \n",
    "        users = batch.users-1 \n",
    "        jokes = batch.jokes-1\n",
    "        \n",
    "        num_ratings += ratings.size()[0]\n",
    "        squared_error += torch.sum((ratings.double() - torch.mm(U, torch.transpose(V, 0, 1))[users.data, jokes.data])**2)\n",
    "        \n",
    "    return ((squared_error/num_ratings)**0.5).data.numpy()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manual gradient with respect to A:\n",
      "[[  1.   1.]\n",
      " [ 12.  12.]\n",
      " [ 18.  18.]]\n",
      "\n",
      "Autograd gradient with respect to A:\n",
      "Variable containing:\n",
      "  1   1\n",
      " 12  12\n",
      " 18  18\n",
      "[torch.DoubleTensor of size 3x2]\n",
      "\n",
      "\n",
      "Manual gradient with respect to B:\n",
      "[[  1.   1.]\n",
      " [ 39.  39.]]\n",
      "\n",
      "Autograd gradient with respect to B:\n",
      "Variable containing:\n",
      "  1   1\n",
      " 39  39\n",
      "[torch.DoubleTensor of size 2x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Toy example to do manual calculation of gradients\n",
    "A = Variable(torch.from_numpy(np.array([[1, 1], [2, 2], [3, 3]])).double(), requires_grad = True)\n",
    "B = Variable(torch.from_numpy(np.array([[1, 1], [2, 2]])).double(), requires_grad = True)\n",
    "person_ids = Variable(torch.from_numpy(np.array([0, 1, 2])).long(), requires_grad = True)\n",
    "song_ids = Variable(torch.from_numpy(np.array([0, 1, 1])).long(), requires_grad = True)\n",
    "rate = Variable(torch.from_numpy(np.array([1, 2, 3])).long())\n",
    "sigma2 = 1.0\n",
    "\n",
    "neg_log_lik = neg_log_likelihood(sigma2, A, B, person_ids, song_ids, rate)\n",
    "neg_log_lik.backward()\n",
    "\n",
    "A_num = A.data.numpy()\n",
    "B_num = B.data.numpy()\n",
    "rate_matrix = np.array([[1, 0], [0, 2], [0, 3]])\n",
    "mult = (-torch.ones(1) * (sigma2)**(-1.0)).numpy()\n",
    "\n",
    "a1_grad = mult * (rate_matrix[0, 0] - A_num[0].reshape(-1, A_num.shape[1]).dot(B_num[0].reshape(B_num.shape[1], -1))) * B_num[0]\n",
    "a2_grad = mult * (rate_matrix[1, 1] - A_num[1].reshape(-1, A_num.shape[1]).dot(B_num[1].reshape(B_num.shape[1], -1))) * B_num[1]\n",
    "a3_grad = mult * (rate_matrix[2, 1] - A_num[2].reshape(-1, A_num.shape[1]).dot(B_num[1].reshape(B_num.shape[1], -1))) * B_num[1]\n",
    "A_grad = np.concatenate([a1_grad, a2_grad, a3_grad], axis = 0)\n",
    "\n",
    "b1_grad = mult * (rate_matrix[0, 0] - A_num[0].reshape(-1, A_num.shape[1]).dot(B_num[0].reshape(B_num.shape[1], -1))) * A_num[0]\n",
    "b2_grad = mult * ((rate_matrix[1, 1] - A_num[1].reshape(-1, A_num.shape[1]).dot(B_num[1].reshape(B_num.shape[1], -1))) * A_num[1] +\\\n",
    "                   (rate_matrix[2, 1] - A_num[2].reshape(-1, A_num.shape[1]).dot(B_num[1].reshape(B_num.shape[1], -1))) * A_num[2])\n",
    "B_grad = np.concatenate([b1_grad, b2_grad], axis = 0)\n",
    "\n",
    "print('Manual gradient with respect to A:\\n' + str(A_grad))\n",
    "print()\n",
    "print('Autograd gradient with respect to A:\\n' + str(A.grad))\n",
    "print()\n",
    "print('Manual gradient with respect to B:\\n' + str(B_grad))\n",
    "print()\n",
    "print('Autograd gradient with respect to B:\\n' + str(B.grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Batch number 100\n",
      "Batch number 200\n",
      "Batch number 300\n",
      "Batch number 400\n",
      "Batch number 500\n",
      "Epoch 1\n",
      "Batch number 100\n",
      "Batch number 200\n",
      "Batch number 300\n",
      "Batch number 400\n",
      "Batch number 500\n",
      "Sigma squared MLE = 1.06888028311\n",
      "Train RMSE: 1.03386666602\n",
      "Val RMSE: 1.04849306749\n"
     ]
    }
   ],
   "source": [
    "# batch size 3000\n",
    "K = 7\n",
    "sigma2 = 1.0\n",
    "num_epochs = 2\n",
    "U = Variable(torch.from_numpy(np.random.rand(highest_user, K).reshape(highest_user, K)), requires_grad = True)\n",
    "V = Variable(torch.from_numpy(np.random.rand(num_jokes, K).reshape(num_jokes, K)), requires_grad = True)\n",
    "optimizer = torch.optim.SGD([U, V], lr = 0.0005)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print('Epoch ' + str(epoch))\n",
    "    batch_num = 0\n",
    "    for batch in train_iter:\n",
    "        batch_num += 1\n",
    "        if batch_num % 100 == 0:\n",
    "            print('Batch number ' + str(batch_num))\n",
    "\n",
    "        ratings = batch.ratings\n",
    "        users = batch.users-1 \n",
    "        jokes = batch.jokes-1\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = neg_log_likelihood(sigma2, U, V, users, jokes, ratings)\n",
    "        loss.backward()\n",
    "        #print(loss.data[0])\n",
    "        optimizer.step()\n",
    "        \n",
    "\n",
    "train_rmse_k = rmse(U, V, train_iter)\n",
    "val_rmse_k = rmse(U, V, val_iter)\n",
    "\n",
    "sigma2_mle = train_rmse_k**2\n",
    "\n",
    "print('Sigma squared MLE = ' + str(sigma2_mle))\n",
    "print('Train RMSE: ' + str(train_rmse_k))\n",
    "print('Val RMSE: ' + str(val_rmse_k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[1.1039178662337159, 1.1282048470046748] #lr = 0.0008, batch_size=3000\n",
    "[1.0573551910309043, 1.0920181002235005] #lr = 0.0008, batch_size=3000\n",
    "[1.04716172563, 1.06281632165] #lr = 0.0008, batch_size=3000\n",
    "[1.04378780926, 1.05934880586] #lr = 0.0005, batch_size=3000\n",
    "[1.04299195183, 1.05527209921] #lr = 0.0005, batch_size=3000\n",
    "[1.03386666602, 1.04849306749] #lr = 0.0005, batch_size=3000\n",
    "[]\n",
    "[]\n",
    "[]\n",
    "[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sigma2 = 1.0\n",
    "K = np.arange(10)\n",
    "optimizer = torch.optim.SGD([U, V], lr = 0.0001)\n",
    "num_epochs = 10\n",
    "train_rmse = []\n",
    "val_rmse = []\n",
    "\n",
    "for k in K:\n",
    "    print('K = ' + str(k+1))\n",
    "    U = Variable(torch.from_numpy(np.random.rand(highest_user, k+1).reshape(highest_user, k+1)), requires_grad = True)\n",
    "    V = Variable(torch.from_numpy(np.random.rand(num_jokes, k+1).reshape(num_jokes, k+1)), requires_grad = True)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch ' + str(epoch))\n",
    "        batch_num = 0\n",
    "        for batch in train_iter:\n",
    "            batch_num += 1\n",
    "            if batch_num % 100 == 0:\n",
    "                print('Batch number ' + str(batch_num))\n",
    "\n",
    "            ratings = batch.ratings \n",
    "            users = batch.users-1 \n",
    "            jokes = batch.jokes-1\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss = neg_log_likelihood(sigma2, U, V, users, jokes, ratings)\n",
    "            loss.backward()\n",
    "            print(loss.data[0])\n",
    "            optimizer.step()\n",
    "            \n",
    "    train_rmse_k = rmse(U, V, train_iter)\n",
    "    val_rmse_k = rmse(U, V, val_iter)\n",
    "    \n",
    "    print('Train RMSE: ' + str(train_rmse_k))\n",
    "    print('Val RMSE: ' + str(val_rmse_k))\n",
    "    \n",
    "    train_rmse.append(train_rmse_k)\n",
    "    val_rmse.append(val_rmse_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(K, train_rmse)\n",
    "plt.plot(K, val_rmse)\n",
    "plt.xlabel('K')\n",
    "plt.ylabel('RMSE')\n",
    "plt.title('RMSE vs. K for Training and Validation Data')\n",
    "plt.legend(labels = ['Training', 'Validation'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# batch size 1000\n",
    "K = 2\n",
    "sigma2 = 1.0\n",
    "num_epochs = 7\n",
    "U = Variable(torch.from_numpy(0.1 * np.random.randn(highest_user, K).reshape(highest_user, K)), requires_grad = True)\n",
    "V = Variable(torch.from_numpy(0.1 * np.random.randn(num_jokes, K).reshape(num_jokes, K)), requires_grad = True)\n",
    "a = Variable(torch.from_numpy(0.1 * np.random.randn(highest_user).reshape(highest_user, -1)), requires_grad = True)\n",
    "b = Variable(torch.from_numpy(0.1 * np.random.randn(num_jokes).reshape(num_jokes, -1)), requires_grad = True)\n",
    "g = Variable(torch.ones(1).double() * 3.5, requires_grad = True)\n",
    "optimizer = torch.optim.Adam([U, V, a, b, g], lr = 0.05)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print('Epoch ' + str(epoch))\n",
    "    batch_num = 0\n",
    "    for batch in train_iter:\n",
    "        batch_num += 1\n",
    "        if batch_num % 100 == 0:\n",
    "            print('Batch number ' + str(batch_num))\n",
    "\n",
    "        ratings = batch.ratings \n",
    "        users = batch.users-1 \n",
    "        jokes = batch.jokes-1\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = neg_log_likelihood_biases(sigma2, U, V, a, b, g, users, jokes, ratings)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "print('Global bias: ' + str(g.data.numpy()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rank jokes and get biases\n",
    "jokeID_bias = np.concatenate([np.arange(1, num_jokes+1).reshape(num_jokes, -1), b.data.numpy()], axis = 1)\n",
    "jokeID_bias[jokeID_bias[:, 1].argsort()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jokes_raw = []\n",
    "for line in open('jester_items.clean.dat', 'r'):\n",
    "    if len(line) > 5:\n",
    "        jokes_raw.append(line.rstrip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_transform = CountVectorizer(input='content', encoding='utf-8', decode_error='strict', strip_accents=None, \\\n",
    "                                                                     lowercase=True, preprocessor=None, tokenizer=None, stop_words=None, \\\n",
    "                                                                     ngram_range=(1, 1), analyzer='word', max_df=1.0, min_df=1, max_features=None, \\\n",
    "                                                                     vocabulary=None, binary=False)\n",
    "\n",
    "joke_feature_vectors = raw_transform.fit_transform(jokes_raw)#.todense()\n",
    "#joke_feature_vectors = np.concatenate([np.ones(num_jokes).reshape(num_jokes, -1), joke_feature_vectors], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3 Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neg_log_posterior(w, x, sigma, b, y, jokes):\n",
    "    features = x[jokes.data.numpy()]\n",
    "    linear = features.dot(w.data.numpy().reshape(w.size()[0], -1))\n",
    "    \n",
    "    lower_cut = b[y.data.numpy() - 1].reshape(y.size()[0], -1)\n",
    "    upper_cut = lower_cut + 2\n",
    "    np.place(upper_cut, upper_cut == 6, 10.0)\n",
    "    np.place(lower_cut, lower_cut == -4, -10.0)\n",
    "    \n",
    "    lower_cdf_input = (lower_cut - linear)/sigma.data.numpy()\n",
    "    upper_cdf_input = (upper_cut - linear)/sigma.data.numpy()\n",
    "    \n",
    "    lower_cdf = norm.cdf(lower_cdf_input, loc=0, scale=1)\n",
    "    upper_cdf = norm.cdf(upper_cdf_input, loc=0, scale=1)\n",
    "\n",
    "    max1 = np.log(1 - np.exp(np.log(lower_cdf) - np.log(upper_cdf))) + np.log(upper_cdf)\n",
    "    max2 = np.log(1 - np.exp(np.log(1 - upper_cdf) - np.log(1 - lower_cdf))) + np.log(1 - lower_cdf)\n",
    "    max_matrix = np.concatenate([max1, max2], axis = 1)\n",
    "    max_log = Variable(torch.from_numpy(np.amax(max_matrix, axis = 1)).double())\n",
    "\n",
    "    return torch.sum(w**2) - torch.sum(max_log)\n",
    "\n",
    "\n",
    "\n",
    "def rmse2(w, x, sigma, b, data_iter):\n",
    "    Xw = x.dot(w.data.numpy().reshape(w.size()[0], -1))\n",
    "    print(Xw)\n",
    "    \n",
    "    cdf = np.ones((x.shape[0], len(b) + 1))\n",
    "    for i in range(cdf.shape[1]):\n",
    "        if i == 0:\n",
    "            cdf[:, i] = norm.cdf((cdf[:, i].reshape(cdf.shape[0], -1) * -10.0 - Xw)/sigma.data.numpy()).reshape(cdf.shape[0], )\n",
    "        elif i == (cdf.shape[1] - 1):\n",
    "            cdf[:, i] = norm.cdf((cdf[:, i].reshape(cdf.shape[0], -1) * 10.0 - Xw)/sigma.data.numpy()).reshape(cdf.shape[0], )\n",
    "        else:\n",
    "            cdf[:, i] = norm.cdf((cdf[:, i].reshape(cdf.shape[0], -1) * b[i] - Xw)/sigma.data.numpy()).reshape(cdf.shape[0], )\n",
    "            \n",
    "    prob_matrix = np.zeros((x.shape[0], len(b)))\n",
    "    for i in range(prob_matrix.shape[1]):\n",
    "        prob_matrix[:, i] = cdf[:, i+1] - cdf[:, i]\n",
    "        \n",
    "    possible_ratings = (np.arange(5) + 1).reshape(5, -1)\n",
    "    \n",
    "    expectation_by_joke = prob_matrix.dot(possible_ratings)\n",
    "    \n",
    "    squared_error = 0.0\n",
    "    num_ratings = 0\n",
    "    for batch in data_iter:\n",
    "        ratings = batch.ratings \n",
    "        users = batch.users-1 \n",
    "        jokes = batch.jokes-1\n",
    "        \n",
    "        print(prob_matrix)\n",
    "        num_ratings += ratings.data.numpy().shape[0]\n",
    "        print(expectation_by_joke[jokes.data.numpy()])\n",
    "        squared_error += np.sum((expectation_by_joke[jokes.data.numpy()] - ratings.data.numpy())**2)\n",
    "        \n",
    "    return (squared_error/num_ratings)**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 2\n",
    "b = np.array([-4, -2, 0, 2, 4])\n",
    "w = Variable(torch.from_numpy(0.1 * np.random.randn(joke_feature_vectors.shape[1]).reshape(joke_feature_vectors.shape[1], -1)), requires_grad = True)\n",
    "sigma = Variable(torch.from_numpy(np.array(np.random.rand(1))), requires_grad = True)\n",
    "optimizer = torch.optim.Adam([w, sigma], lr = 0.001)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print('Epoch ' + str(epoch))\n",
    "    batch_num = 0\n",
    "    for batch in train_iter:\n",
    "        batch_num += 1\n",
    "        if batch_num % 100 == 0:\n",
    "            print('Batch number ' + str(batch_num))\n",
    "\n",
    "        ratings = batch.ratings \n",
    "        users = batch.users-1 \n",
    "        jokes = batch.jokes-1\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = neg_log_posterior(w, joke_feature_vectors, sigma, b, ratings, jokes)\n",
    "        loss.backward()\n",
    "        print(loss.data[0])\n",
    "        optimizer.step()\n",
    "\n",
    "print('Test RMSE: ' + str(rmse2(w, joke_feature_vectors, sigma, b, test_iter)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse2(w, joke_feature_vectors, sigma, b, test_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma.grad"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5.3",
   "language": "python",
   "name": "py35"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
