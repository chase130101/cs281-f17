{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats\n",
    "import scipy.integrate\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from utils import load_imdb\n",
    "from utils import bag_of_words\n",
    "from utilsCPU import load_imdbCPU\n",
    "from utilsCPU import bag_of_wordsCPU\n",
    "\n",
    "\n",
    "# import jtplot\n",
    "from jupyterthemes import jtplot\n",
    "\n",
    "# you can select an alternative theme's plot style by name\n",
    "# oceans16 | grade3 | chesterish | onedork | monokai | solarizedl\n",
    "jtplot.style('onedork')\n",
    "\n",
    "# set \"context\" (paper, notebook, talk, or poster)\n",
    "# & font scale (scalar applied to labels, legend, etc.)\n",
    "jtplot.style('grade3', context='paper', fscale=1.4)\n",
    "\n",
    "# turn on X- and Y-axis tick marks (default=False)\n",
    "# and turn off the axis grid lines (default=True)\n",
    "jtplot.style(ticks=True, grid=False)\n",
    "\n",
    "# set the default figure size\n",
    "# x (length), y (height)\n",
    "jtplot.figsize(x=6., y=5.)\n",
    "\n",
    "# or just adjust the aspect ratio\n",
    "# new_length = length * aspect\n",
    "jtplot.figsize(aspect=1.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def chi_density(x, D):\n",
    "    return 2*x*scipy.stats.chi2.pdf(x**2, D)\n",
    "\n",
    "x = np.linspace(0.1, 14, 1000)\n",
    "D_list = [1, 5, 10, 20, 40, 60, 80, 100]\n",
    "for i in D_list:\n",
    "    plt.plot(x, chi_density(x, i))\n",
    "    plt.xlim([0, 14])\n",
    "    plt.ylim([0, 0.8])\n",
    "    plt.title('Chi Distributions with Different Degrees of Freedom')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('Density')\n",
    "    plt.legend('top right', labels = ['D = ' + str(i) for i in D_list])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def chi_CDF(D, lower_end_point, upper_end_point):\n",
    "    return scipy.integrate.quad(chi_density, lower_end_point, upper_end_point, args = (D))\n",
    "\n",
    "x = np.linspace(0.1, 14, 1000)\n",
    "D = 100\n",
    "cdf_vals = np.zeros(len(x))\n",
    "for i in range(len(x)):\n",
    "    cdf_vals[i] = chi_CDF(D, 0.1, x[i])[0]\n",
    "\n",
    "plt.plot(x, cdf_vals)\n",
    "plt.xlim([0, 14])\n",
    "plt.ylim([0, 1.01])\n",
    "plt.title('Chi CDF when D = 100')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('P(X < x)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problems 4, 5, 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 2000\n",
    "train_iter, val_iter, test_iter, text_field = load_imdbCPU(imdb_path='imdb.zip', imdb_dir='imdb', batch_size=batch_size, gpu=False, reuse=True,\\\n",
    "                                                        repeat=False, shuffle=True)\n",
    "V = len(text_field.vocab)\n",
    "num_labels = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def NB_train(train_iter, class_labels_dir_prior_params, vocab_length, num_labels, beta0_prior_params=None, beta1_prior_params=None, dir0_unif_prior_param=None, dir1_unif_prior_param=None, maxD=None,\\\n",
    "             betaPrior=True):\n",
    "    class_labels_dir_posterior_params = torch.addcmul(torch.zeros(num_labels).double(), value=1, tensor1=torch.ones(num_labels).double(), \\\n",
    "                                                      tensor2=class_labels_dir_prior_params)\n",
    "    if betaPrior == True:\n",
    "        beta00_posterior_vec = torch.ones(vocab_length).double()*beta0_prior_params[0]\n",
    "        beta10_posterior_vec = torch.ones(vocab_length).double()*beta0_prior_params[1]\n",
    "        beta01_posterior_vec = torch.ones(vocab_length).double()*beta1_prior_params[0]\n",
    "        beta11_posterior_vec = torch.ones(vocab_length).double()*beta1_prior_params[1]\n",
    "        for batch in train_iter:\n",
    "            x = bag_of_wordsCPU(batch, text_field).double()\n",
    "            y = torch.from_numpy(torch.unsqueeze((batch.label - 1).double(), 1).data.numpy()).double()\n",
    "\n",
    "            x = x.div(x)\n",
    "            x[x != x] = 0\n",
    "            \n",
    "            class_labels_dir_posterior_params.add_(torch.DoubleTensor([sum(1 - y).numpy(), sum(y).numpy()]))\n",
    "            #beta00_posterior_vec.add_(torch.matmul(torch.transpose(1 - x, 0, 1), 1 - y))\n",
    "            #beta10_posterior_vec.add_(torch.matmul(torch.transpose(x, 0, 1), 1 - y))\n",
    "            #beta01_posterior_vec.add_(torch.matmul(torch.transpose(1 - x, 0, 1), y))\n",
    "            #beta11_posterior_vec.add_(torch.matmul(torch.transpose(x, 0, 1), y))\n",
    "            for i in range(len(y)):\n",
    "                if y[i].numpy() == 0:\n",
    "                    beta00_posterior_vec.add_(1 - x[i])\n",
    "                    beta10_posterior_vec.add_(x[i])\n",
    "\n",
    "                if y[i].numpy() == 1:\n",
    "                    beta01_posterior_vec.add_(1 - x[i])\n",
    "                    beta11_posterior_vec.add_(x[i])\n",
    "\n",
    "        class_label_dir_posterior_expectation = class_labels_dir_posterior_params/sum(class_labels_dir_posterior_params)\n",
    "        beta0_posterior_expectation = beta10_posterior_vec/(beta00_posterior_vec + beta10_posterior_vec)\n",
    "        beta1_posterior_expectation = beta11_posterior_vec/(beta01_posterior_vec + beta11_posterior_vec)\n",
    "\n",
    "        return class_label_dir_posterior_expectation, beta0_posterior_expectation, beta1_posterior_expectation\n",
    "    \n",
    "    \n",
    "    if betaPrior == False:\n",
    "        \n",
    "        count = 0\n",
    "        dir0_posterior_matrix = np.zeros((maxD + 1, vocab_length)) + dir0_unif_prior_param\n",
    "        dir1_posterior_matrix = np.zeros((maxD + 1, vocab_length)) + dir1_unif_prior_param\n",
    "        \n",
    "        for batch in train_iter:\n",
    "            \n",
    "            print(count, end = ' ')\n",
    "            count += 1\n",
    "                \n",
    "            x = bag_of_wordsCPU(batch, text_field).numpy()\n",
    "            y = torch.from_numpy(torch.unsqueeze((batch.label - 1).double(), 1).data.numpy()).double()\n",
    "        \n",
    "            class_labels_dir_posterior_params.add_(torch.DoubleTensor([sum(1 - y).numpy(), sum(y).numpy()]))\n",
    "            \n",
    "            y = (batch.label - 1).data.numpy()\n",
    "            x0 = x[np.where(y == 0), :][0]\n",
    "            x1 = x[np.where(y == 1), :][0]\n",
    "            \n",
    "            for i in range(x.shape[1]):\n",
    "\n",
    "                vals0 =  np.unique(x0[:, i], return_counts = True)[0]\n",
    "                counts0 = np.unique(x0[:, i], return_counts = True)[1]\n",
    "                vals1 =  np.unique(x1[:, i], return_counts = True)[0]\n",
    "                counts1 = np.unique(x1[:, i], return_counts = True)[1]\n",
    "\n",
    "                vals0_maxD = vals0[np.where(vals0 < maxD)].tolist()\n",
    "                counts0_maxD = counts0[np.where(vals0 < maxD)].tolist()\n",
    "                vals1_maxD = vals1[np.where(vals1 < maxD)].tolist()\n",
    "                counts1_maxD = counts1[np.where(vals1 < maxD)].tolist()\n",
    "\n",
    "                vals0_maxD.append(maxD)\n",
    "                counts0_maxD.append(np.sum(counts0[np.where(vals0 >= maxD)]))\n",
    "                vals1_maxD.append(maxD)\n",
    "                counts1_maxD.append(np.sum(counts1[np.where(vals1 >= maxD)]))\n",
    "                \n",
    "                dir0_posterior_matrix[:, i][np.array(vals0_maxD).astype(int)] += np.array(counts0_maxD)\n",
    "                dir1_posterior_matrix[:, i][np.array(vals1_maxD).astype(int)] += np.array(counts1_maxD)\n",
    "        \n",
    "        class_label_dir_posterior_expectation = class_labels_dir_posterior_params/sum(class_labels_dir_posterior_params)\n",
    "        dir0_posterior_expectation = torch.from_numpy(np.divide(dir0_posterior_matrix, np.sum(dir0_posterior_matrix, axis = 0))).double()\n",
    "        dir1_posterior_expectation = torch.from_numpy(np.divide(dir1_posterior_matrix, np.sum(dir1_posterior_matrix, axis = 0))).double()\n",
    "        \n",
    "        return class_label_dir_posterior_expectation, dir0_posterior_expectation, dir1_posterior_expectation\n",
    "                                     \n",
    "        \n",
    "        #for i in range(len(y)):\n",
    "        #    if y[i].numpy() == 0:\n",
    "        #        for j in range(len(x[i])):\n",
    "        #            if x[i, j].numpy() < maxD:\n",
    "        #                dir0_posterior_matrix[x[i, j], j].add_(1)\n",
    "        #            else:\n",
    "        #                dir0_posterior_matrix[maxD, j].add_(1)\n",
    "                        \n",
    "        #    if y[i].numpy() == 1:\n",
    "        #        for j in range(len(x[i])):\n",
    "        #            if x[i, j].numpy() < maxD:\n",
    "        #                dir0_posterior_matrix[x[i, j], j].add_(1)\n",
    "        #            else:\n",
    "        #                dir0_posterior_matrix[maxD, j].add_(1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        #dir0_posterior_vec = torch.addcmul(torch.zeros(vocab_length).double(), value=1, tensor1=torch.ones(vocab_length).double(), \\\n",
    "        #                                   tensor2=dir0_prior_params)\n",
    "        #dir1_posterior_vec = torch.addcmul(torch.zeros(vocab_length).double(), value=1, tensor1=torch.ones(vocab_length).double(), \\\n",
    "        #                                   tensor2=dir1_prior_params)\n",
    "        \n",
    "        #for batch in train_iter:\n",
    "        #    x = bag_of_words(batch, text_field).double()\n",
    "        #    y = torch.from_numpy(torch.unsqueeze((batch.label - 1).double(), 1).data.numpy()).double()\n",
    "            \n",
    "        #    class_labels_dir_posterior_params.add_(torch.DoubleTensor([sum(1 - y).numpy(), sum(y).numpy()]))\n",
    "            #dir0_posterior_vec.add_(torch.matmul(torch.transpose(x, 0, 1), 1 - y))\n",
    "            #dir1_posterior_vec.add_(torch.matmul(torch.transpose(x, 0, 1), y))\n",
    "        #    for i in range(len(y)):\n",
    "        #        if y[i].numpy() == 0:\n",
    "        #            dir0_posterior_vec.add_(x[i])\n",
    "\n",
    "        #        if y[i].numpy() == 1:\n",
    "        #            dir1_posterior_vec.add_(x[i])\n",
    "        \n",
    "        #class_label_dir_posterior_expectation = class_labels_dir_posterior_params/sum(class_labels_dir_posterior_params)\n",
    "        #dir0_posterior_expectation = dir0_posterior_vec/sum(dir0_posterior_vec)\n",
    "        #dir1_posterior_expectation = dir1_posterior_vec/sum(dir1_posterior_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_accuracy(dataset, vocab_length, class_label_dir_posterior_expectation, beta0_posterior_expectation=None, beta1_posterior_expectation=None, dir0_posterior_expectation=None, \\\n",
    "                     dir1_posterior_expectation=None, maxD=None, betaPrior = True):\n",
    "    test_num_correct = torch.zeros(1).double()\n",
    "    size_test_data = torch.zeros(1).double()\n",
    "    if betaPrior == True:\n",
    "        for batch in dataset:\n",
    "            x = bag_of_wordsCPU(batch, text_field).double()\n",
    "            y = torch.from_numpy((batch.label - 1).double().data.numpy()).double() # batch.label is 1/2, while we want 0/1\n",
    "\n",
    "            xi1_ind_class0 = torch.matmul(x, torch.log(beta0_posterior_expectation))\n",
    "            xi0_ind_class0 = torch.matmul(1.0 - x, torch.log(1.0 - beta0_posterior_expectation))\n",
    "            log_likelihood_class0 = torch.unsqueeze(torch.log(class_label_dir_posterior_expectation)[0] + xi1_ind_class0 + xi1_ind_class0, 1)\n",
    "\n",
    "            xi1_ind_class1 = torch.matmul(x, torch.log(beta1_posterior_expectation))\n",
    "            xi0_ind_class1 = torch.matmul(1.0 - x, torch.log(1.0 - beta1_posterior_expectation))\n",
    "            log_likelhood_class1 = torch.unsqueeze(torch.log(class_label_dir_posterior_expectation)[1] + xi1_ind_class1 + xi1_ind_class1, 1)\n",
    "\n",
    "            test_num_correct.add_(sum(torch.from_numpy(np.argmax(torch.cat([log_likelihood_class0, \\\n",
    "                                                                            log_likelhood_class1], 1).numpy(), axis = 1)).double() == y))\n",
    "            size_test_data.add_(len(y))\n",
    "\n",
    "        return test_num_correct/size_test_data\n",
    "    \n",
    "    if betaPrior == False:\n",
    "        \n",
    "        log_dir0_posterior_expectation = np.log(dir0_posterior_expectation.numpy())\n",
    "        log_dir1_posterior_expectation = np.log(dir1_posterior_expectation.numpy())\n",
    "        \n",
    "        for batch in dataset:\n",
    "            x = bag_of_wordsCPU(batch, text_field).numpy()\n",
    "            y = torch.from_numpy((batch.label - 1).double().data.numpy()).double() # batch.label is 1/2, while we want 0/1\n",
    "            \n",
    "            \n",
    "            for i in range(x.shape[0]):\n",
    "                log_likelhood_class0 = np.log(class_label_dir_posterior_expectation.numpy()[0])\n",
    "                log_likelhood_class1 = np.log(class_label_dir_posterior_expectation.numpy()[1])\n",
    "                \n",
    "                np.place(x[i], x[i] > 10, 10)\n",
    "                \n",
    "                log_likelhood_class0 = np.sum(log_dir0_posterior_expectation[x[i].astype(int), np.arange(vocab_length)]) + np.log(class_label_dir_posterior_expectation.numpy()[0])\n",
    "                log_likelhood_class1 = np.sum(log_dir1_posterior_expectation[x[i].astype(int), np.arange(vocab_length)]) + np.log(class_label_dir_posterior_expectation.numpy()[1])\n",
    "                \n",
    "                y_pred = float(np.array(np.argmax(np.array([log_likelhood_class0, log_likelhood_class1]))))\n",
    "                \n",
    "                test_num_correct.add_(y_pred == y[i])\n",
    "                size_test_data.add_(1.0)\n",
    "                                   \n",
    "            return test_num_correct/size_test_data\n",
    "                \n",
    "                                             \n",
    "            #log_likelihood_class0 = torch.unsqueeze(torch.log(class_label_dir_posterior_expectation)[0] + torch.matmul(x, torch.log(dir0_posterior_expectation)), 1)\n",
    "            #log_likelhood_class1 = torch.unsqueeze(torch.log(class_label_dir_posterior_expectation)[1] + torch.matmul(x, torch.log(dir1_posterior_expectation)), 1)\n",
    "\n",
    "            #test_num_correct.add_(sum(torch.from_numpy(np.argmax(torch.cat([log_likelihood_class0, \\\n",
    "            #                                                                log_likelhood_class1], 1).numpy(), axis = 1)).double() == y))\n",
    "            #size_test_data.add_(len(y))\n",
    "\n",
    "        #return test_num_correct/size_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Christopher\\Anaconda3\\envs\\py35\\lib\\site-packages\\ipykernel_launcher.py:15: UserWarning: other is not broadcastable to self, but they have the same number of elements.  Falling back to deprecated pointwise behavior.\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Christopher\\Anaconda3\\envs\\py35\\lib\\site-packages\\ipykernel_launcher.py:50: UserWarning: other is not broadcastable to self, but they have the same number of elements.  Falling back to deprecated pointwise behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2 3 4 5 6 7 8 9 10 11 \n",
      "Val accuracy w/ beta class-conditional prior using uniform hyperparams of magnitude 0.01: 0.855\n",
      "Val accuracy w/ Dirichlet class-conditional prior using uniform hyperparams of magnitude 0.01: 0.843\n",
      "\n",
      "0 1 2 3 4 5 6 7 8 9 10 11 \n",
      "Val accuracy w/ beta class-conditional prior using uniform hyperparams of magnitude 0.05: 0.86\n",
      "Val accuracy w/ Dirichlet class-conditional prior using uniform hyperparams of magnitude 0.05: 0.854\n",
      "\n",
      "0 1 2 3 4 5 6 7 8 9 10 11 \n",
      "Val accuracy w/ beta class-conditional prior using uniform hyperparams of magnitude 0.1: 0.863\n",
      "Val accuracy w/ Dirichlet class-conditional prior using uniform hyperparams of magnitude 0.1: 0.852\n",
      "\n",
      "0 1 2 3 4 5 6 7 8 9 10 11 \n",
      "Val accuracy w/ beta class-conditional prior using uniform hyperparams of magnitude 0.4: 0.865\n",
      "Val accuracy w/ Dirichlet class-conditional prior using uniform hyperparams of magnitude 0.4: 0.873\n",
      "\n",
      "0 1 2 3 4 5 6 7 8 9 10 11 \n",
      "Val accuracy w/ beta class-conditional prior using uniform hyperparams of magnitude 0.7: 0.863\n",
      "Val accuracy w/ Dirichlet class-conditional prior using uniform hyperparams of magnitude 0.7: 0.869\n",
      "\n",
      "0 1 2 3 4 5 6 7 8 9 10 11 \n",
      "Val accuracy w/ beta class-conditional prior using uniform hyperparams of magnitude 1.0: 0.867\n",
      "Val accuracy w/ Dirichlet class-conditional prior using uniform hyperparams of magnitude 1.0: 0.871\n",
      "\n",
      "Test accuracy w/ beta class-conditional prior using best uniform hyperparams (magnitude=1.0): 0.864\n",
      "Test accuracy w/ Dirichlet class-conditional prior using best uniform hyperparams (magnitude=0.4): 0.873\n"
     ]
    }
   ],
   "source": [
    "prior_param_magnitudes = [0.01, 0.05, 0.1, 0.4, 0.7, 1.0]\n",
    "maxD = 10\n",
    "val_accuracies_beta = torch.zeros(len(prior_param_magnitudes)).double()\n",
    "val_accuracies_dir = torch.zeros(len(prior_param_magnitudes)).double()\n",
    "max_val_accuracy_beta = torch.zeros(1).double()\n",
    "max_val_accuracy_dir = torch.zeros(1).double\n",
    "\n",
    "for i in range(len(prior_param_magnitudes)):\n",
    "    \n",
    "    beta0_prior_params = torch.DoubleTensor([prior_param_magnitudes[i], prior_param_magnitudes[i]])\n",
    "    beta1_prior_params = torch.DoubleTensor([prior_param_magnitudes[i], prior_param_magnitudes[i]])\n",
    "    class_labels_dir_prior_params = torch.DoubleTensor([1, 1])\n",
    "    class_label_dir_posterior_expectation, beta0_posterior_expectation, beta1_posterior_expectation = NB_train(train_iter, class_labels_dir_prior_params, V, num_labels, \\\n",
    "                                                                                                              beta0_prior_params, beta1_prior_params)\n",
    "\n",
    "    dir0_prior_params = torch.ones(V).double()*prior_param_magnitudes[i]\n",
    "    dir1_prior_params = torch.ones(V).double()*prior_param_magnitudes[i]\n",
    "    class_labels_dir_prior_params = torch.DoubleTensor([1, 1])\n",
    "    class_label_dir_posterior_expectation, dir0_posterior_expectation, dir1_posterior_expectation = NB_train(train_iter, class_labels_dir_prior_params, V, num_labels, \\\n",
    "                                                                                                    dir0_unif_prior_param=prior_param_magnitudes[i], dir1_unif_prior_param=prior_param_magnitudes[i], \\\n",
    "                                                                                                    maxD=maxD, betaPrior=False)\n",
    "    \n",
    "    val_accuracies_beta[i] = compute_accuracy(val_iter, V, class_label_dir_posterior_expectation, beta0_posterior_expectation, beta1_posterior_expectation)[0]\n",
    "    \n",
    "    val_accuracies_dir[i] = compute_accuracy(val_iter, V, class_label_dir_posterior_expectation, dir0_posterior_expectation=dir0_posterior_expectation, \\\n",
    "                                             dir1_posterior_expectation=dir1_posterior_expectation, maxD=maxD, betaPrior=False)[0]\n",
    "    \n",
    "    if i == 0:\n",
    "        max_val_accuracy_beta = val_accuracies_beta[0]\n",
    "        max_val_accuracy_dir = val_accuracies_dir[0]\n",
    "        best_beta_prior_param_magnitudes = prior_param_magnitudes[0]\n",
    "        best_dir_prior_param_magnitudes = prior_param_magnitudes[0]\n",
    "    \n",
    "    if i > 0:\n",
    "        if max_val_accuracy_beta < val_accuracies_beta[i]:\n",
    "            max_val_accuracy_beta = val_accuracies_beta[i]\n",
    "            best_beta_prior_param_magnitudes = prior_param_magnitudes[i]\n",
    "            test_accuracy_best_beta_prior = compute_accuracy(test_iter, V, class_label_dir_posterior_expectation, beta0_posterior_expectation, beta1_posterior_expectation)[0]\n",
    "        \n",
    "        if max_val_accuracy_dir < val_accuracies_dir[i]:\n",
    "            max_val_accuracy_dir = val_accuracies_dir[i]\n",
    "            best_dir_prior_param_magnitudes = prior_param_magnitudes[i]\n",
    "            test_accuracy_best_dir_prior = compute_accuracy(test_iter, V, class_label_dir_posterior_expectation, dir0_posterior_expectation=dir0_posterior_expectation, \\\n",
    "                                                            dir1_posterior_expectation=dir1_posterior_expectation, maxD=maxD, betaPrior=False)[0]\n",
    "    print()        \n",
    "    print('Val accuracy w/ beta class-conditional prior using uniform hyperparams of magnitude ' + str(prior_param_magnitudes[i]) + ': ' + str(val_accuracies_beta[i]))\n",
    "    print('Val accuracy w/ Dirichlet class-conditional prior using uniform hyperparams of magnitude ' + str(prior_param_magnitudes[i]) + ': ' + str(val_accuracies_dir[i]))\n",
    "    print()\n",
    "\n",
    "print('Test accuracy w/ beta class-conditional prior using best uniform hyperparams (magnitude=' + \\\n",
    "      str(best_beta_prior_param_magnitudes) + '): ' + str(test_accuracy_best_beta_prior))\n",
    "print('Test accuracy w/ Dirichlet class-conditional prior using best uniform hyperparams (magnitude=' + \\\n",
    "      str(best_dir_prior_param_magnitudes) + '): ' + str(test_accuracy_best_dir_prior))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 5b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "train_iter, val_iter, test_iter, text_field = load_imdb(imdb_path='imdb.zip', imdb_dir='imdb', batch_size=batch_size, gpu=True, reuse=True,\\\n",
    "                                                        repeat=False, shuffle=True)\n",
    "V = len(text_field.vocab) # vocab size\n",
    "num_labels = 2\n",
    "vocab_list = text_field.vocab.itos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_model(input_dim, output_dim):\n",
    "    model = torch.nn.Sequential()\n",
    "    # computes w_c^T x + b_c \n",
    "    model.add_module(\"linear\", torch.nn.Linear(input_dim, output_dim).cuda())\n",
    "    # Compute our log softmax term.\n",
    "    model.add_module(\"softmax\", torch.nn.LogSoftmax().cuda())\n",
    "    return model\n",
    "\n",
    "\n",
    "def l1_logistic_loss(model, lambda_, fx, y):\n",
    "    log_loss = torch.nn.NLLLoss(size_average = True)\n",
    "    log_loss = log_loss.forward(fx, y)\n",
    "    \n",
    "    lasso_part = torch.nn.L1Loss(size_average = False)\n",
    "    params = next(model.parameters())\n",
    "    target = Variable(torch.zeros(params.size()[0], params.size()[1]).cuda(), requires_grad=False)\n",
    "    lasso_part = lasso_part.forward(params, target)\n",
    "    \n",
    "    return log_loss + lasso_part * lambda_\n",
    "    \n",
    "    \n",
    "def train(model, lambda_, x, y, optimizer):\n",
    "    # Resets the gradients to 0\n",
    "    optimizer.zero_grad()\n",
    "    # Computes the function above. (log softmax w_c^T x + b_c)\n",
    "    fx = model.forward(x)\n",
    "    # Computes a loss. Gives a scalar. \n",
    "    loss = l1_logistic_loss(model, lambda_, fx, y)\n",
    "    # Magically computes the gradients. \n",
    "    loss.backward()\n",
    "    # updates the weights\n",
    "    optimizer.step()\n",
    "    return loss.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Lambda = 0\n",
      "Epoch train accuracy: 0.664833333333\n",
      "Epoch train loss: 208.94907519221306\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-a202e0a50d28>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain_iter\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbag_of_words\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext_field\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m             \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabel\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;31m# batch.label is 1/2, while we want 0/1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\OneDrive\\Documents\\Harvard Grad school\\2017-2018 Academic Year\\Fall 2017\\Advanced Machine Learning (CS 281)\\T2_noGit\\utils.py\u001b[0m in \u001b[0;36mbag_of_words\u001b[1;34m(batch, text_field)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mbag_of_words\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext_field\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m     \u001b[0mV\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext_field\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mV\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m     \u001b[0mones\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\py35\\lib\\site-packages\\torch\\_utils.py\u001b[0m in \u001b[0;36m_cuda\u001b[1;34m(self, device, async)\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[0mnew_type\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mnew_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0masync\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lam_vals = [0, 0.001, 0.01, 0.1, 1]\n",
    "num_epochs = 15\n",
    "\n",
    "for lam in lam_vals:\n",
    "    model = build_model(V, num_labels)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "    size_val_data = 0.0\n",
    "    val_num_correct = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        size_training_data = 0.0\n",
    "        train_num_correct = 0.0\n",
    "        loss = 0.0\n",
    "\n",
    "        for batch in train_iter:\n",
    "            x = Variable(bag_of_words(batch, text_field))\n",
    "            y = batch.label - 1 # batch.label is 1/2, while we want 0/1\n",
    "\n",
    "            batch_loss = train(model, lam, x, y, optimizer)\n",
    "            loss += batch_loss\n",
    "\n",
    "            batch_num_correct = np.sum(np.argmax(torch.exp(model.forward(x)).data.cpu().numpy(), axis = 1) == y.data.cpu().numpy())\n",
    "            train_num_correct += batch_num_correct\n",
    "            size_training_data += len(y)\n",
    "        \n",
    "        print('Epoch ' + str(epoch + 1))\n",
    "        print('Lambda = ' + str(lam))\n",
    "        print('Epoch train accuracy: ' + str(train_num_correct/size_training_data))\n",
    "        print('Epoch train loss: ' + str(loss))\n",
    "        print()\n",
    "        \n",
    "    \n",
    "    for batch in val_iter:\n",
    "        x = Variable(bag_of_words(batch, text_field))\n",
    "        y = batch.label - 1 # batch.label is 1/2, while we want 0/1\n",
    "        \n",
    "        batch_num_correct = np.sum(np.argmax(torch.exp(model.forward(x)).data.cpu().numpy(), axis = 1) == y.data.cpu().numpy())\n",
    "        val_num_correct += batch_num_correct\n",
    "        size_val_data += len(y)\n",
    "        \n",
    "    print('Lambda = ' + str(lam))\n",
    "    print('Val accuracy: ' + str(val_num_correct/size_val_data))\n",
    "    print()\n",
    "\n",
    "    \n",
    "    test_num_correct = 0.0\n",
    "    size_test_data = 0.0\n",
    "    for batch in test_iter:\n",
    "        x = Variable(bag_of_words(batch, text_field))\n",
    "        y = batch.label - 1 # batch.label is 1/2, while we want 0/1\n",
    "\n",
    "        batch_num_correct = np.sum(np.argmax(torch.exp(model.forward(x)).data.cpu().numpy(), axis = 1) == y.data.cpu().numpy())\n",
    "        test_num_correct += batch_num_correct\n",
    "        size_test_data += len(y)\n",
    "\n",
    "    print('Lambda: ' + str(lam))\n",
    "    print('Test accuracy: ' + str(test_num_correct/size_test_data))\n",
    "    print()\n",
    "\n",
    "    model_params = next(model.parameters())\n",
    "    print('Words with highest valued coefficients for predicting class 0: ' + str(np.flip(np.array(vocab_list)[np.argsort(model_params[0].data.cpu().numpy())[-5:]], axis = 0)))\n",
    "    print('Words with lowest valued coefficients for predicting class 0: ' + str(np.array(vocab_list)[np.argsort(model_params[0].data.cpu().numpy())[0:5]]))\n",
    "    print('Words with highest valued coefficients for predicting class 1: ' + str(np.flip(np.array(vocab_list)[[np.argsort(model_params[1].data.cpu().numpy())[-5:]]], axis = 0)))\n",
    "    print('Words with lowest valued coefficients for predicting class 1: ' + str(np.array(vocab_list)[[np.argsort(model_params[1].data.cpu().numpy())[0:5]]]))\n",
    "    print()\n",
    "    \n",
    "    for j in range(num_labels):\n",
    "        abs_params = np.absolute(model_params.data.cpu().numpy()[j])\n",
    "        print('Number of 0 valued parameters for class ' + str(j) + ' (lambda=' + str(lam) + '): ' + str(len(abs_params[abs_params < 10.0**(-4)])/V))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_model_neural(input_dim1, output_dim1, output_dim2):\n",
    "    model = torch.nn.Sequential()\n",
    "    # computes w_c^T x + b_c \n",
    "    model.add_module(\"linear1\", torch.nn.Linear(input_dim1, output_dim1).cuda())\n",
    "    model.add_module('tanh', torch.nn.Tanh().cuda())\n",
    "    model.add_module(\"linear2\", torch.nn.Linear(output_dim1, output_dim2).cuda())\n",
    "    # Compute our log softmax term.\n",
    "    model.add_module(\"softmax\", torch.nn.LogSoftmax().cuda())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = build_model_neural(V, int(V/1000.0), num_labels)\n",
    "loss_func = torch.nn.NLLLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "num_epochs = 15\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    size_training_data = 0.0\n",
    "    num_correct = 0.0\n",
    "    loss = 0.0\n",
    "    \n",
    "    for batch in train_iter:\n",
    "        x = Variable(bag_of_words(batch, text_field))\n",
    "        y = batch.label - 1 # batch.label is 1/2, while we want 0/1\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        fx = model.forward(x)\n",
    "        L = loss_func(fx, y)\n",
    "        L.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss += L.data[0]\n",
    "        batch_num_correct = np.sum(np.argmax(torch.exp(fx).data.cpu().numpy(), axis = 1) == y.data.cpu().numpy())\n",
    "        num_correct += batch_num_correct\n",
    "        size_training_data += len(y)\n",
    "        \n",
    "    print('Epoch ' + str(epoch + 1))\n",
    "    print('Epoch train accuracy: ' + str(num_correct/size_training_data))\n",
    "    print('Epoch loss: ' + str(loss))\n",
    "    print()\n",
    "    \n",
    "    \n",
    "test_num_correct = 0.0\n",
    "size_test_data = 0.0\n",
    "for batch in test_iter:\n",
    "    x = Variable(bag_of_words(batch, text_field))\n",
    "    y = batch.label - 1 # batch.label is 1/2, while we want 0/1\n",
    "\n",
    "    batch_num_correct = np.sum(np.argmax(torch.exp(model.forward(x)).data.cpu().numpy(), axis = 1) == y.data.cpu().numpy())\n",
    "    test_num_correct += batch_num_correct\n",
    "    size_test_data += len(y)\n",
    "\n",
    "print('Test accuracy: ' + str(test_num_correct/size_test_data))\n",
    "print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5.3",
   "language": "python",
   "name": "py35"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
